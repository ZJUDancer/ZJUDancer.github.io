<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZJUDancer&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.zjudancer.club/"/>
  <updated>2018-12-09T10:35:18.542Z</updated>
  <id>http://www.zjudancer.club/</id>
  
  <author>
    <name>ZJUDancer</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Improvements in the last year</title>
    <link href="http://www.zjudancer.club/2018/11/28/RoboCup-2019-Sydney/"/>
    <id>http://www.zjudancer.club/2018/11/28/RoboCup-2019-Sydney/</id>
    <published>2018-11-28T14:56:41.000Z</published>
    <updated>2018-12-09T10:35:18.542Z</updated>
    
    <content type="html"><![CDATA[<p>This year we made a large progress in image processing and gait control, which contributes to the more intelligent robots.</p><h2 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h2><h3 id="Electrical-Specifications"><a href="#Electrical-Specifications" class="headerlink" title="Electrical Specifications"></a>Electrical Specifications</h3><p>​    This year, NVIDIA Jetson TX2 are adopted as our main controller.. Motor controller and camera controller are merged into a single controller due to the better computing performance of Jetson TX2. The motor part executes the movements of all directions and maintains the balance, while the camera part works on object detection, self-localization, strategies, and multi-robot communications. To meet the requirement of a smaller chest size, we redesign the circuit board and make the total electrical system slimmer. Moreover, we adopt a new foot sensor board for walking more steadily. </p><h3 id="Mechanical-Specifications"><a href="#Mechanical-Specifications" class="headerlink" title="Mechanical Specifications"></a>Mechanical Specifications</h3><p>​    This year we mainly modify three parts in the mechanical structure of our robots. Firstly, we unify the size of our robots to simplify the design and manufacture of the components. Besides, we reduce the thickness of the total thorax and remove the unnecessary components which are used to connect the different circuit boards. Another significant improvement is that we reduce the spacing between the two hip-joints. Since we need enough power to support the motion of our robot, we move one of the batteries which used to be between two hip-joints to the back of our robot. Although this change results in a higher center of gravity, it seems that the gait of the robot is not considerably affected.</p><p>​    Our robot has two legs, two arms, one trunk, and one head, which belongs to a classical planning. We have two specifications of our robot: one is of about 620 mm height and the other is nearly 700 mm which has longer legs and neck. Each robot is driven by 18 servo motors: six in each leg, two in each arm and the last two in the head. The six-leg-servos allow flexible leg movements. Three orthogonal servos constitute the 3-DOF hip-joint. Two orthogonal servos form the 2-DOF ankle joint. One servo drives the knee joint. The motor distribution is different but the DOF is the same.</p><h3 id="Sensors-specification"><a href="#Sensors-specification" class="headerlink" title="Sensors specification"></a>Sensors specification</h3><ul><li><p>Servo</p><p>DYNAMIXEL MX-64 and MX-28 with joint angle feedback, which benefits to the closed-loop control.</p></li><li><p>IMU</p><p>Analog device ADIS16355 featured with tri-axis gyroscope, and tri-axis accelerometer, which conduces to keep the balance of our robot.</p></li><li><p>Image sensor</p><p>OmniVision OV2710 with 150-degree FOV, which provides a wider view angle, and improves the perception efficiency.</p></li></ul><h2 id="Software"><a href="#Software" class="headerlink" title="Software"></a>Software</h2><p>​    Our software architecture remain almost the same this year. The main work is to enhance the modules including the vision module, localization module, behavior module, etc. Detailed improvements we have made are introduced in the following subsections.</p><h3 id="Cognition"><a href="#Cognition" class="headerlink" title="Cognition"></a>Cognition</h3><p>​    In order to use the goalposts to assist in self-localization, we have improved its detection. Since the traditional method takes vast computing resources and can not provide accurate results, we attempt to recognize goal, robots, and balls at the same time using a deep learning approach. However, the detection of a whole goal is defective, and our robots can hardly ever observe it. Thus, we only detect the bottom of goalposts and get fairly reliable results.</p><h3 id="Calibration"><a href="#Calibration" class="headerlink" title="Calibration"></a>Calibration</h3><p>​    Calibrating extrinsic parameters is an essential but laborious process after transportation or hardware modification. The previous extrinsic calibration method requires an empty field to take various pictures and label preset points on every picture manually, which takes a lot of time and manpower. This year, we adopt the aruco marker and let robots perform an automatic detection and labeling. Although the recall rate is not very high, the re-projection error of labeled point becomes much lower and the precision is almost 100%. </p><h3 id="Navigation"><a href="#Navigation" class="headerlink" title="Navigation"></a>Navigation</h3><p>​    Last year, the self-localization of our robots mostly depended on the center circle, which lead to a wrong location estimation and deviated shooting direction. This year, since the recognition of goalposts has been improved, we add them as reliable landmarks for the particle filter. Furthermore, we make additional improvements in the resampling part so that the location estimation can be more robust. As a result, our robots are able to achieve a high-precision self-localization in the playing field.</p><h3 id="Behavior"><a href="#Behavior" class="headerlink" title="Behavior"></a>Behavior</h3><p>​    This year, we refactor our behavior framework by a standard behavior tree. The basic logic is almost the same as the previous version, but the whole architecture is more clear and convenient for modifying and adding new features quickly. In addition, we add a new defender role which patrols on our own half and stares at the center circle to ensure its location estimation accuracy. Moreover, we try to add some team play features such as sharing the ball position and avoiding the collision when multiple robots seeing the ball at the same time. However, due to some network defects, these features didn’t work in RoboCup 2018. We will continue to fix such problems and improve the stability of our strategy.</p><h3 id="Motion"><a href="#Motion" class="headerlink" title="Motion"></a>Motion</h3><p>​    Since the previous version of the gait generation algorithm had fewer advantages compared with other teams during the RoboCup 2018, we decide to refactor our motion module this year. The motion module mainly consists of following four child modules:</p><ul><li><p>IO: </p><p>IO module manages the external devices including servo, IMU and foot pressure sensor. This module provides an interface to send instructions to actuators or get readings from sensors.</p></li><li><p>Kinematics</p><p>Inverse kinematics module is used for calculating the joint angles when the robot acts in a known gesture. We can get the analytic solution of one leg by some geometry methods. Based on that, we can get two leg joint angles when the robot is standing on the ground. We also use forward kinematics to estimate the position of the center of mass of the robot. Getting the estimation helps the observation of the state of the robot for controller designing.</p></li><li><p>State Manager</p><p>This module is used to manage the robot task state. Generally, robots have to cope with unexpected situations, such as collision and falling down. State manager helps to estimate what situation the robot has come cross and give appropriate action command. It is also the interface between behavior module and motion module.</p></li><li><p>Trajectory Generation</p><p>This module is used to plan discrete footprints and generate smooth trajectories for joints, we are going to develop a new parameter tuning graphical interface to customize some piecewise cubic spline curve via determining the points’ position and slope artificially. We will try some intelligent control algorithms to modify these curves if the development well proceeds.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This year we made a large progress in image processing and gait control, which contributes to the more intelligent robots.&lt;/p&gt;
&lt;h2 id=&quot;Ha
      
    
    </summary>
    
    
      <category term="RoboCup" scheme="http://www.zjudancer.club/tags/RoboCup/"/>
    
      <category term="Humanoid" scheme="http://www.zjudancer.club/tags/Humanoid/"/>
    
      <category term="ZJUDancer" scheme="http://www.zjudancer.club/tags/ZJUDancer/"/>
    
  </entry>
  
  <entry>
    <title>Introduction.</title>
    <link href="http://www.zjudancer.club/2017/10/23/Introduction/"/>
    <id>http://www.zjudancer.club/2017/10/23/Introduction/</id>
    <published>2017-10-23T12:53:03.000Z</published>
    <updated>2018-11-28T14:30:53.105Z</updated>
    
    <content type="html"><![CDATA[<p>The robots developed by ZJUDancer for RoboCup 2017 are fully autonomous humanoid robots which play dierent parts as a team in the football game. During the past few years, we won the champions of RoboCup China Open 2007, 2009, 2010, 2011, 2012, and 2013 and advanced to quarter-nals in RoboCup 2012 Mexico and RoboCup 2013 Netherlands. In RoboCup 2015, 2016 and 2017, We won the second place.</p><h2 id="Overview-of-system"><a href="#Overview-of-system" class="headerlink" title="Overview of system"></a>Overview of system</h2><p>The robots developed by ZJUDancer for RoboCup 2017 are fully autonomous humanoid robots which play different parts as a team in the football game.</p><h2 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h2><h3 id="Compute-Unit"><a href="#Compute-Unit" class="headerlink" title="Compute Unit"></a>Compute Unit</h3><p>Our robots are equipped with NVIDIA Jetson Tegra X1, a supercomputer with powerful GPU. It features NVIDIA Maxwell architecture, 256 NVIDIA CUDA cores, and 64-bit CPUs. The deep learning algorithm is highly demanding on the GPU. And CUDA acceleration provided by TX1 make it possible on our robots.</p><h3 id="Sensors-specification"><a href="#Sensors-specification" class="headerlink" title="Sensors specification"></a>Sensors specification</h3><ul><li>Servo: DYNAMIXEL mx-64, mx-28, with joint angle feedback, it helps close-loop control.</li><li>IMU: Analog device ADIS16355. Featured with tri-axis gryoscope, and tri-axis accelerometer. It helps to keep balance of our robot.</li><li>Image sensor: We changed our camera from Logitech C930 to OmniVision OV2710 with 150 degree Fov. It provides wider view angle, and helps improve the efficiency of perception.</li></ul><h2 id="Software"><a href="#Software" class="headerlink" title="Software"></a>Software</h2><p>This year we largely refactored our software system. In order to decrease the coupling of different modules, we rewired all modules using publish-subscribe pattern. The network middleware we use is ZMQ, as it supports many different languages, we implement Behavior modules in Lua, and other modules in C++, all connected to each other seamlessly.</p><p>In our robots, we have 5 process running simultaneously. The vision process runs at 30Hz, and processing incoming images. The motion process runs at 10Hz and deals with local motion. The localization process and beahvior process runs at the same rate with vision process. Lastly, the network process runs at 5Hz to process network messages.</p><h3 id="Vision"><a href="#Vision" class="headerlink" title="Vision"></a>Vision</h3><h4 id="Camera-Module"><a href="#Camera-Module" class="headerlink" title="Camera Module"></a>Camera Module</h4><p>we use libjpeg-turbo to decompress the JEPG images into RGB images.</p><h4 id="Image-processing"><a href="#Image-processing" class="headerlink" title="Image processing"></a>Image processing</h4><p>Last year, we used a ICF(integral channel feature) classifier trained by Adaboost, and detect using sliding window. Since it only detects gray distribution without geometry informations, detecting the ball under complex environment like nearing the goal post is error-prone. Furthermore, it’s very sensitive to the lighting condition.</p><p>This year, in order to solve the questions above, we applied CNN in object detection with the NVIDIA Jetson TX1. Basically, we use Darknet framework and YOLO to detect the ball, goal post and robots. In order to get higher performance, we are trying to implement a independent inference method on darknet aside from the forward propagation using methods like fp16. Because of the motion of the robot, the images are highly blurred, so we used a Kalman filter to get a stable ball position.</p><p>Color segmentation is applied to distract all the white points on the field for the usage of self-localization.</p><h3 id="Navigation"><a href="#Navigation" class="headerlink" title="Navigation"></a>Navigation</h3><p>In order to get close to the target more quickly and avoid obstacles, last year we did some experiment on our mobile platform. Later we’ll try to implement it on our humanoid robot. The experiment is highly involved with ROS(Robot Operating System) and RhIO library.</p><p>Firstly, an developed an improved global path planning algorithm based on generalized voronoi diagram and Hybrid A* algorithm is implemented. Compared with the traditional global path planning algorithm, the method takes into account the minimum turning radius which is geometric constraint of the robots, and introduces the Reeds-Sheep optimal trajectory which skips a large number of open research spaces to improve the algorithm speed. Finally, we use gradient descent optimization to make the final path more smooth and reasonable.</p><p>Secondly, a path tracking method based on Nonlinear Model Predictive Control (NMPC) is proposed. The trajectory tracking method based on NMPC takes into account the model information of the robot itself and improves the accuracy of trajectory tracking. The control period of trajecotory planning can be kept within 40ms at all times.</p><p>As shown above, it’s easy and essential to test and verify the algorithm in mobile robots and we have done a good experiment in mobile robots. Next step, we will verify the algorithm in our biped robots.</p><h3 id="Behavior"><a href="#Behavior" class="headerlink" title="Behavior"></a>Behavior</h3><p>Our behaviour model is based on behaviour tree, implemented in Lua. It reads information from the localization module and gamecontroller module, plans movement for every frame. This year we added new ball searching strategy.</p><h3 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h3><p>Our behaviour model is based on behaviour tree, implemented in Lua. It reads information from the localization module and gamecontroller module, plans movement for every frame. This year we added new ball searching strategy.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The robots developed by ZJUDancer for RoboCup 2017 are fully autonomous humanoid robots which play dierent parts as a team in the footbal
      
    
    </summary>
    
    
      <category term="RoboCup" scheme="http://www.zjudancer.club/tags/RoboCup/"/>
    
      <category term="Humanoid" scheme="http://www.zjudancer.club/tags/Humanoid/"/>
    
      <category term="ZJUDancer" scheme="http://www.zjudancer.club/tags/ZJUDancer/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://www.zjudancer.club/2016/11/17/hello-world/"/>
    <id>http://www.zjudancer.club/2016/11/17/hello-world/</id>
    <published>2016-11-17T15:33:33.000Z</published>
    <updated>2018-11-28T15:59:22.177Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
